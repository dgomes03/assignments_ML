# -*- coding: utf-8 -*-
"""Assignment_1_Diogo_Gomes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ohTAti1VnnKac1-rjuruf3eZLlTYsa4p
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# load the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
wine_data = pd.read_csv(url, sep=';')

# extract features and target
X = wine_data.drop(columns=["quality"]).values
y = wine_data["quality"].values.reshape(-1, 1)

# normalize features
scaler = StandardScaler() # this step ensures less bias to features with bigger scales.
X = scaler.fit_transform(X)

# converting to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

# defining model
class WineQualityModel(nn.Module):
    def __init__(self, input_size, output_size):
        super(WineQualityModel, self).__init__()
        self.linear = nn.Linear(input_size, output_size)

    def forward(self, x):
        return self.linear(x)

# defining model parameters
input_size = X.shape[1]
output_size = 1
model = WineQualityModel(input_size, output_size)

# loss function and optimizer
loss_function = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# training loop
epochs = 200
loss_history = []

for epoch in range(epochs):
    # forward pass
    predictions = model(X_tensor)
    loss = loss_function(predictions, y_tensor) # loss calculation
    loss_history.append(loss.item()) # keeps record of loss value

    # backward pass
    optimizer.zero_grad() # zero_grad resets the gradients before next optimization step
    loss.backward() # "backward" computes the gradients of the loss using backpropagation
    optimizer.step()

    # print loss every 10 epochs
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

# predictions output
with torch.no_grad(): # no gradient calculation
    y_pred = model(X_tensor).numpy()

# plot actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y, y_pred, alpha=0.5)
plt.xlabel("Actual Quality")
plt.ylabel("Predicted Quality")
plt.title("Actual vs Predicted Wine Quality")
plt.show()

# plot loss vs epochs
plt.figure(figsize=(8, 6))
plt.plot(range(epochs), loss_history, label="Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss vs Epochs")
plt.legend()
plt.show()